{"backend_state":"init","connection_file":"/projects/f5f38597-12ca-4603-94d4-a86293c3f1cc/.local/share/jupyter/runtime/kernel-cdce44ff-64d2-42f0-9b30-275f18f62130.json","kernel":"python3-ubuntu","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"a29eae","input":"# Step 6 - Create a hypothesis about the relationship between some of the feature data and the target. Do you think the feature data can be used to predict survival?","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d7e1b5","input":"# Step 2 - Which column do you think is our target (the variable we are trying to predict)?","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"a8970d","input":"# Step 4 -  How do you think age affects passengers' chances of survival? (hint: to visually investigate correlation we tend to use scatter plots in data science, but the nature of our target data is binary which makes it more difficult. Try to make an age distribution of the passengers (using a histogram) that survived and see which age ranges seemed to do better.)\n\ntitanic_survived = titanic[titanic[\"Survived\"]==1]\nplt.figure(figsize=(8,10))\nplt.style.use('dark_background')\nsns.set_style('darkgrid')\nax = sns.countplot(y='Age', data=titanic_survived)","output":{"0":{"data":{"image/png":"246984cfa62734b9958746fcfaa3c400913d9a15","text/plain":"<Figure size 576x720 with 1 Axes>"},"exec_count":10,"metadata":{"image/png":{"height":588,"width":503},"needs_background":"light"},"output_type":"execute_result"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"d31f76","input":"# Step 5 - What is the age distribution of the people who didn't survive? (hint: to determine the distribution of data, we love to use histograms in data science -- try using multiple data frames.)\ntitanic_did_not_survive = titanic[titanic[\"Survived\"]==0]\nplt.figure(figsize=(8,10))\nplt.style.use('dark_background')\nsns.set_style('darkgrid')\nax = sns.countplot(y='Age', data=titanic_did_not_survive)","output":{"0":{"data":{"image/png":"f596f4e18ca0370eadc8bb81a650086a0e2dc0aa","text/plain":"<Figure size 576x720 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":588,"width":503}},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"1ba7d4","input":"# Convert the categorical variables into dummy variables.\ntitanic['Sex'].replace(to_replace=[\"female\", \"male\"], value=[0, 1], inplace=True)\ntitanic.head(5)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Siblings/Spouses Aboard</th>\n      <th>Parents/Children Aboard</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. Owen Harris Braund</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Miss. Laina Heikkinen</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. William Henry Allen</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Survived  Pclass                                               Name  Sex  \\\n0         0       3                             Mr. Owen Harris Braund    1   \n1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...    0   \n2         1       3                              Miss. Laina Heikkinen    0   \n3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle    0   \n4         0       3                            Mr. William Henry Allen    1   \n\n    Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n0  22.0                        1                        0   7.2500  \n1  38.0                        1                        0  71.2833  \n2  26.0                        0                        0   7.9250  \n3  35.0                        1                        0  53.1000  \n4  35.0                        0                        0   8.0500  "},"exec_count":12,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"f1a23e","input":"titanic_final = titanic[[\"Survived\", \"Age\", \"Pclass\", \"Sex\"]]            #Pclass, Sex, Age, Survived\ntitanic_final.head(10)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>22.0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>26.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>35.0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>27.0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>54.0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>27.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>14.0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Survived   Age  Pclass  Sex\n0         0  22.0       3    1\n1         1  38.0       1    0\n2         1  26.0       3    0\n3         1  35.0       1    0\n4         0  35.0       3    1\n5         0  27.0       3    1\n6         0  54.0       1    1\n7         0   2.0       3    1\n8         1  27.0       3    0\n9         1  14.0       2    0"},"exec_count":13,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"ec32d1","input":"inputs = [\"Pclass\",\"Sex\",\"Age\"]\nx_train, x_test, y_train, y_test = train_test_split(titanic_final[inputs], titanic_final[\"Survived\"], test_size=0.25)","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"c43207","input":"x_train.shape #665 training observations","output":{"0":{"data":{"text/plain":"(665, 3)"},"exec_count":15,"output_type":"execute_result"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"18047a","input":"x_test.shape #222 testing observations.","output":{"0":{"data":{"text/plain":"(222, 3)"},"exec_count":16,"output_type":"execute_result"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"981aa6","input":"k_list = [k for k in range(1,30)] # k_list = [1,2,3,4, ..... , 28,29] [for later]","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"2dc753","input":"# comparing y_hat to y_test - model.score will give us the accuracy. only use this for a single instance of the model so use the best k here!\n# k = 15  is this the best k?\n\n#create a for loop that creates a knn for every single k in k_list and adds this score to the scores dictionary.\nscores = {}\n\nfor i in k_list:\n    #training\n    model_1 = knn(n_neighbors = i) # test for k = [0 -> 30]\n    model_1.fit(x_train, y_train)\n    #testing\n    y_hat = model_1.predict(x_test)\n    #y_test is the actual value, and y_hat is our predicted value. This is how confusion matrices are formed! y_hat = y_test, our model predicted right. If they aren't equal then our model is wrong.\n    score = model_1.score(x_test, y_test) # accuracy on the dataset.\n    # add this score to the scores dictionary: dict[key] = value\n    scores[i] = score*100\n    \nprint(scores)\nprint(max(scores.values()))","output":{"0":{"name":"stdout","output_type":"stream","text":"{1: 74.77477477477478, 2: 78.37837837837837, 3: 80.63063063063063, 4: 81.08108108108108, 5: 78.82882882882883, 6: 79.27927927927928, 7: 80.18018018018019, 8: 76.12612612612612, 9: 78.37837837837837, 10: 77.92792792792793, 11: 79.27927927927928, 12: 75.22522522522522, 13: 76.12612612612612, 14: 75.22522522522522, 15: 73.87387387387388, 16: 72.07207207207207, 17: 72.52252252252252, 18: 69.81981981981981, 19: 72.52252252252252, 20: 69.36936936936937, 21: 69.81981981981981, 22: 69.81981981981981, 23: 70.72072072072072, 24: 70.72072072072072, 25: 69.81981981981981, 26: 68.01801801801803, 27: 69.36936936936937, 28: 68.01801801801803, 29: 68.01801801801803}\n81.08108108108108\n"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"c9422e","input":"k = 12 #12,3,25\n#training\nmodel_1 = knn(n_neighbors = k)\nmodel_1.fit(x_train, y_train)\n#testing\ny_hat = model_1.predict(x_test)\n#y_test is the actual value, and y_hat is our predicted value. This is how confusion matrices are formed! y_hat = y_test, our model predicted right. If they aren't equal then our model is wrong.\nscore = model_1.score(x_test, y_test) # accuracy on the dataset.\nprint(score)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.7522522522522522\n"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"935e78","input":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier as knn\n\n#try changing this at the end of the project\n#plt.style.use('ggplot')","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"77b787","input":"from sklearn import svm\nkernlist = ['linear', 'poly', 'rbf', 'sigmoid']\nscores = {}\n#create a for loop that checks each kernel and see what the kernel is.\nfor i in kernlist:\n    #create model\n    titanic_model2 = svm.SVC(kernel=i)\n    #fit\n    titanic_model2.fit(x_train, y_train)\n    #predict\n    y_hat = titanic_model2.predict(x_test)\n    #score\n    score = titanic_model2.score(x_test, y_test)\n    scores[i] = score*100\nprint(scores)\nscore = max(scores.values())\nprint(max(scores.values()))","output":{"0":{"name":"stdout","output_type":"stream","text":"{'linear': 80.18018018018019, 'poly': 62.612612612612615, 'rbf': 57.65765765765766, 'sigmoid': 51.35135135135135}\n80.18018018018019\n"}},"pos":43,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"2f422d","input":"score","output":{"0":{"data":{"text/plain":"80.18018018018019"},"exec_count":21,"output_type":"execute_result"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"8c1ad4","input":"# Finish this block\nfrom sklearn.linear_model import LogisticRegression as LR\n#create model (don't use any arguments) and fit\ntitanic_model3 = LR().fit(x_train, y_train)\n\n#predict\ny_hat = titanic_model3.predict(x_test)\n\n#score\nscore = titanic_model3.score(x_test, y_test)*100\nprint(score)\n\n#no need for a for loop","output":{"0":{"name":"stdout","output_type":"stream","text":"81.53153153153153\n"}},"pos":46,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"4de5c5","input":"from sklearn.metrics import confusion_matrix, classification_report\ny_true = y_test\ny_pred = model_1.predict(x_test)\ncmatrix1=confusion_matrix(y_true, y_pred)","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"c732f4","input":"sns.heatmap(cmatrix1, annot=True, fmt='d')","output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f140d3307f0>"},"exec_count":24,"output_type":"execute_result"},"1":{"data":{"image/png":"f90c7b83fb8846f7b4d27473eac406c865e0a4aa","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":24,"metadata":{"image/png":{"height":411,"width":645}},"output_type":"execute_result"}},"pos":49,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"ebb592","input":"print(classification_report(y_true, y_pred))","output":{"0":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.71      0.95      0.82       128\n           1       0.88      0.48      0.62        94\n\n    accuracy                           0.75       222\n   macro avg       0.80      0.72      0.72       222\nweighted avg       0.78      0.75      0.73       222\n\n"}},"pos":50,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"f5d629","input":"y_true = y_test\ny_pred = titanic_model2.predict(x_test)\ncmatrix2=confusion_matrix(y_true, y_pred)","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"9e4df1","input":"sns.heatmap(cmatrix2, annot=True, fmt='d')","output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f140d0ac1c0>"},"exec_count":27,"output_type":"execute_result"},"1":{"data":{"image/png":"6656fa49f3c0ce32d9f123cbec6b32132ca52d5e","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":411,"width":638}},"output_type":"execute_result"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":28,"id":"026871","input":"print(classification_report(y_true, y_pred))","output":{"0":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.57      0.66      0.61       128\n           1       0.41      0.32      0.36        94\n\n    accuracy                           0.51       222\n   macro avg       0.49      0.49      0.48       222\nweighted avg       0.50      0.51      0.50       222\n\n"}},"pos":54,"type":"cell"}
{"cell_type":"code","exec_count":29,"id":"4a83b5","input":"y_true = y_test\ny_pred = titanic_model3.predict(x_test)\ncmatrix3=confusion_matrix(y_true, y_pred)","pos":56,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"d8a3f8","input":"# Step 1 Load the `titanic_data.csv` file into a pandas dataframe\ntitanic=pd.read_csv('titanic_data.csv')\n","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"f7e6a3","input":"sns.heatmap(cmatrix3, annot=True, fmt='d')","output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f140d03f3d0>"},"exec_count":30,"output_type":"execute_result"},"1":{"data":{"image/png":"2c01bd6d7cd1bbcecdafd4c56340872a5ea77cc1","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":30,"metadata":{"image/png":{"height":411,"width":645}},"output_type":"execute_result"}},"pos":57,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"f38fc0","input":"print(classification_report(y_true, y_pred))","output":{"0":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.80      0.91      0.85       128\n           1       0.85      0.68      0.76        94\n\n    accuracy                           0.82       222\n   macro avg       0.82      0.80      0.80       222\nweighted avg       0.82      0.82      0.81       222\n\n"}},"pos":58,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"47295e","input":"# Step 3 - Which feature columns do you think will be good predictors of our target for machine learning?\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\n"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"f3ddaa","input":" from sklearn.ensemble import RandomForestClassifier\n#here\n#create model\ntitanic_model4 = RandomForestClassifier(n_estimators=500, max_depth=5)\n\n#fit model\ntitanic_model4.fit(x_train, y_train)\n\n#predict\ny_hat=titanic_model4.predict(x_test)\n\n#score\nscore = titanic_model4.score(x_test, y_test)\nprint(score)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8558558558558559\n"}},"pos":62,"type":"cell"}
{"cell_type":"code","exec_count":35,"id":"3da9bf","input":"from sklearn.tree import plot_tree\nn = 499\nfig = plt.figure(figsize=(30, 30))\nplot_tree(titanic_model4.estimators_[n], feature_names=inputs, filled=True, impurity=True, rounded=True)","output":{"0":{"data":{"text/plain":"[Text(883.5, 1494.8999999999999, 'Sex <= 0.5\\ngini = 0.464\\nsamples = 422\\nvalue = [422, 243]'),\n Text(441.75, 1223.1, 'Pclass <= 2.5\\ngini = 0.396\\nsamples = 147\\nvalue = [64, 171]'),\n Text(139.5, 951.3, 'Age <= 23.5\\ngini = 0.108\\nsamples = 80\\nvalue = [7, 115]'),\n Text(93.0, 679.4999999999999, 'gini = 0.0\\nsamples = 13\\nvalue = [0, 23]'),\n Text(186.0, 679.4999999999999, 'Age <= 27.5\\ngini = 0.131\\nsamples = 67\\nvalue = [7, 92]'),\n Text(93.0, 407.6999999999998, 'Pclass <= 1.5\\ngini = 0.332\\nsamples = 11\\nvalue = [4, 15]'),\n Text(46.5, 135.89999999999986, 'gini = 0.375\\nsamples = 3\\nvalue = [1, 3]'),\n Text(139.5, 135.89999999999986, 'gini = 0.32\\nsamples = 8\\nvalue = [3, 12]'),\n Text(279.0, 407.6999999999998, 'Pclass <= 1.5\\ngini = 0.072\\nsamples = 56\\nvalue = [3, 77]'),\n Text(232.5, 135.89999999999986, 'gini = 0.073\\nsamples = 37\\nvalue = [2, 51]'),\n Text(325.5, 135.89999999999986, 'gini = 0.071\\nsamples = 19\\nvalue = [1, 26]'),\n Text(744.0, 951.3, 'Age <= 27.5\\ngini = 0.5\\nsamples = 67\\nvalue = [57, 56]'),\n Text(558.0, 679.4999999999999, 'Age <= 20.5\\ngini = 0.47\\nsamples = 44\\nvalue = [28, 46]'),\n Text(465.0, 407.6999999999998, 'Age <= 6.5\\ngini = 0.498\\nsamples = 31\\nvalue = [26, 23]'),\n Text(418.5, 135.89999999999986, 'gini = 0.298\\nsamples = 7\\nvalue = [2, 9]'),\n Text(511.5, 135.89999999999986, 'gini = 0.465\\nsamples = 24\\nvalue = [24, 14]'),\n Text(651.0, 407.6999999999998, 'Age <= 25.0\\ngini = 0.147\\nsamples = 13\\nvalue = [2, 23]'),\n Text(604.5, 135.89999999999986, 'gini = 0.091\\nsamples = 10\\nvalue = [1, 20]'),\n Text(697.5, 135.89999999999986, 'gini = 0.375\\nsamples = 3\\nvalue = [1, 3]'),\n Text(930.0, 679.4999999999999, 'Age <= 38.0\\ngini = 0.381\\nsamples = 23\\nvalue = [29, 10]'),\n Text(837.0, 407.6999999999998, 'Age <= 30.75\\ngini = 0.469\\nsamples = 15\\nvalue = [15, 9]'),\n Text(790.5, 135.89999999999986, 'gini = 0.165\\nsamples = 6\\nvalue = [10, 1]'),\n Text(883.5, 135.89999999999986, 'gini = 0.473\\nsamples = 9\\nvalue = [5, 8]'),\n Text(1023.0, 407.6999999999998, 'Age <= 62.5\\ngini = 0.124\\nsamples = 8\\nvalue = [14, 1]'),\n Text(976.5, 135.89999999999986, 'gini = 0.0\\nsamples = 7\\nvalue = [14, 0]'),\n Text(1069.5, 135.89999999999986, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n Text(1325.25, 1223.1, 'Age <= 10.0\\ngini = 0.279\\nsamples = 275\\nvalue = [358, 72]'),\n Text(1116.0, 951.3, 'Age <= 0.915\\ngini = 0.484\\nsamples = 12\\nvalue = [7, 10]'),\n Text(1069.5, 679.4999999999999, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 4]'),\n Text(1162.5, 679.4999999999999, 'Pclass <= 2.5\\ngini = 0.497\\nsamples = 10\\nvalue = [7, 6]'),\n Text(1116.0, 407.6999999999998, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n Text(1209.0, 407.6999999999998, 'Age <= 8.0\\ngini = 0.486\\nsamples = 9\\nvalue = [7, 5]'),\n Text(1162.5, 135.89999999999986, 'gini = 0.346\\nsamples = 7\\nvalue = [7, 2]'),\n Text(1255.5, 135.89999999999986, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 3]'),\n Text(1534.5, 951.3, 'Age <= 75.5\\ngini = 0.255\\nsamples = 263\\nvalue = [351, 62]'),\n Text(1488.0, 679.4999999999999, 'Age <= 24.5\\ngini = 0.246\\nsamples = 262\\nvalue = [351, 59]'),\n Text(1395.0, 407.6999999999998, 'Pclass <= 2.5\\ngini = 0.162\\nsamples = 87\\nvalue = [123, 12]'),\n Text(1348.5, 135.89999999999986, 'gini = 0.311\\nsamples = 20\\nvalue = [21, 5]'),\n Text(1441.5, 135.89999999999986, 'gini = 0.12\\nsamples = 67\\nvalue = [102, 7]'),\n Text(1581.0, 407.6999999999998, 'Pclass <= 1.5\\ngini = 0.283\\nsamples = 175\\nvalue = [228, 47]'),\n Text(1534.5, 135.89999999999986, 'gini = 0.426\\nsamples = 50\\nvalue = [54, 24]'),\n Text(1627.5, 135.89999999999986, 'gini = 0.206\\nsamples = 125\\nvalue = [174, 23]'),\n Text(1581.0, 679.4999999999999, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 3]')]"},"exec_count":35,"output_type":"execute_result"},"1":{"data":{"image/png":"2b59471f49bedf39de9ab62e7e1f5cb24931c568","text/plain":"<Figure size 2160x2160 with 1 Axes>"},"exec_count":35,"metadata":{"image/png":{"height":1645,"width":1697}},"output_type":"execute_result"}},"pos":64,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"83ae15","input":"# Step 2 Find out how many passengers are in the file. One trick is to use the count function!\ntitanic.value_counts()\ntitanic.count()","output":{"0":{"data":{"text/plain":"Survived                   887\nPclass                     887\nName                       887\nSex                        887\nAge                        887\nSiblings/Spouses Aboard    887\nParents/Children Aboard    887\nFare                       887\ndtype: int64"},"exec_count":4,"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"4e3905","input":"# Step 3 List all of the \"features\" in the file\ntitanic.info()","output":{"0":{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 887 entries, 0 to 886\nData columns (total 8 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Survived                 887 non-null    int64  \n 1   Pclass                   887 non-null    int64  \n 2   Name                     887 non-null    object \n 3   Sex                      887 non-null    object \n 4   Age                      887 non-null    float64\n 5   Siblings/Spouses Aboard  887 non-null    int64  \n 6   Parents/Children Aboard  887 non-null    int64  \n 7   Fare                     887 non-null    float64\ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.6+ KB\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"edc001","input":"# Step 4 Use pandas'.describe() function to learn about the dataset.\ntitanic.describe()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>Siblings/Spouses Aboard</th>\n      <th>Parents/Children Aboard</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>887.000000</td>\n      <td>887.000000</td>\n      <td>887.000000</td>\n      <td>887.000000</td>\n      <td>887.000000</td>\n      <td>887.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.385569</td>\n      <td>2.305524</td>\n      <td>29.471443</td>\n      <td>0.525366</td>\n      <td>0.383315</td>\n      <td>32.30542</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.487004</td>\n      <td>0.836662</td>\n      <td>14.121908</td>\n      <td>1.104669</td>\n      <td>0.807466</td>\n      <td>49.78204</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.250000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.92500</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.45420</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.13750</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.32920</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         Survived      Pclass         Age  Siblings/Spouses Aboard  \\\ncount  887.000000  887.000000  887.000000               887.000000   \nmean     0.385569    2.305524   29.471443                 0.525366   \nstd      0.487004    0.836662   14.121908                 1.104669   \nmin      0.000000    1.000000    0.420000                 0.000000   \n25%      0.000000    2.000000   20.250000                 0.000000   \n50%      0.000000    3.000000   28.000000                 0.000000   \n75%      1.000000    3.000000   38.000000                 1.000000   \nmax      1.000000    3.000000   80.000000                 8.000000   \n\n       Parents/Children Aboard       Fare  \ncount               887.000000  887.00000  \nmean                  0.383315   32.30542  \nstd                   0.807466   49.78204  \nmin                   0.000000    0.00000  \n25%                   0.000000    7.92500  \n50%                   0.000000   14.45420  \n75%                   0.000000   31.13750  \nmax                   6.000000  512.32920  "},"exec_count":6,"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"ff3c02","input":"# Step 5 Use pandas' .head() function to check out the first few rows.\ntitanic.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Siblings/Spouses Aboard</th>\n      <th>Parents/Children Aboard</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. Owen Harris Braund</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Miss. Laina Heikkinen</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. William Henry Allen</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Survived  Pclass                                               Name  \\\n0         0       3                             Mr. Owen Harris Braund   \n1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n2         1       3                              Miss. Laina Heikkinen   \n3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n4         0       3                            Mr. William Henry Allen   \n\n      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n0    male  22.0                        1                        0   7.2500  \n1  female  38.0                        1                        0  71.2833  \n2  female  26.0                        0                        0   7.9250  \n3  female  35.0                        1                        0  53.1000  \n4    male  35.0                        0                        0   8.0500  "},"exec_count":7,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"858b5e","input":"# Step 6 Use pandas' .tail() function to check out the last 10 rows.\ntitanic.tail(10)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Siblings/Spouses Aboard</th>\n      <th>Parents/Children Aboard</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>877</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. Johann Markun</td>\n      <td>male</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8958</td>\n    </tr>\n    <tr>\n      <th>878</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Miss. Gerda Ulrika Dahlberg</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5167</td>\n    </tr>\n    <tr>\n      <th>879</th>\n      <td>0</td>\n      <td>2</td>\n      <td>Mr. Frederick James Banfield</td>\n      <td>male</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5000</td>\n    </tr>\n    <tr>\n      <th>880</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. Henry Jr Sutehall</td>\n      <td>male</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0500</td>\n    </tr>\n    <tr>\n      <th>881</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mrs. William (Margaret Norton) Rice</td>\n      <td>female</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>29.1250</td>\n    </tr>\n    <tr>\n      <th>882</th>\n      <td>0</td>\n      <td>2</td>\n      <td>Rev. Juozas Montvila</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n    </tr>\n    <tr>\n      <th>883</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Miss. Margaret Edith Graham</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Miss. Catherine Helen Johnston</td>\n      <td>female</td>\n      <td>7.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n    </tr>\n    <tr>\n      <th>885</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Mr. Karl Howell Behr</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Mr. Patrick Dooley</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"     Survived  Pclass                                 Name     Sex   Age  \\\n877         0       3                    Mr. Johann Markun    male  33.0   \n878         0       3          Miss. Gerda Ulrika Dahlberg  female  22.0   \n879         0       2         Mr. Frederick James Banfield    male  28.0   \n880         0       3                Mr. Henry Jr Sutehall    male  25.0   \n881         0       3  Mrs. William (Margaret Norton) Rice  female  39.0   \n882         0       2                 Rev. Juozas Montvila    male  27.0   \n883         1       1          Miss. Margaret Edith Graham  female  19.0   \n884         0       3       Miss. Catherine Helen Johnston  female   7.0   \n885         1       1                 Mr. Karl Howell Behr    male  26.0   \n886         0       3                   Mr. Patrick Dooley    male  32.0   \n\n     Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n877                        0                        0   7.8958  \n878                        0                        0  10.5167  \n879                        0                        0  10.5000  \n880                        0                        0   7.0500  \n881                        0                        5  29.1250  \n882                        0                        0  13.0000  \n883                        0                        0  30.0000  \n884                        1                        2  23.4500  \n885                        0                        0  30.0000  \n886                        0                        0   7.7500  "},"exec_count":8,"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"8f1878","input":"# Step 1 - Explore the data provided (e.g., looking at pandas statistics (ie. using value_counts(), histograms, scatter plots, etc.)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntitanic['Pclass'].value_counts()","output":{"0":{"data":{"text/plain":"3    487\n1    216\n2    184\nName: Pclass, dtype: int64"},"exec_count":9,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"markdown","id":"01a445","input":"#### Model 3: Logistic Regression\nTo get the hang of creating models, now create a logistic regression model with no starting parameters on the data: [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"22eac9","input":"Step 3: Write answer here using markdown. sex, age, Pclass","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"25b0f2","input":"## Problem 4: Classification Machine Learning","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"368513","input":"#### Notes from this section:","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"3e218b","input":"Step 2: Write answer here using markdown.\nSurvived/ whether the passenger lived or not, basically 1 or 0.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"493978","input":"## Problem 2: Explore the Data\n1. Explore the data provided (e.g., looking at pandas statistics (ie. using value_counts(), histograms, scatter plots, etc.)\n2. Which column do you think is our target (the variable we are trying to predict)?\n3. Which feature columns do you think will be good predictors of our target for machine learning?\n4. How do you think age affects passengers' chances of survival?\n5. What is the age distribution of survivors?\n6. Create a hypothesis about the relationship between some of the feature data and the target. Do you think the feature data can be used to predict survival?","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"4e8626","input":"##### Model 3 - Logistic Regression","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"52e62e","input":"#### Model 2: Support Vector Machines\nCreate a support vector machine model with each type of kernel to find out the kernel that yields the best accuracy for this project. ","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"58f91c","input":"#### Classification Models in Supervised Machine Learning\nFor this assignment, we will be focusing on classification, so we need to research some types of [machine learning models](https://scikit-learn.org/stable/supervised_learning.html) that we can use for classification. Here is a guide that we can start looking at for choosing our models based on problem type: [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). From here we can start preprocessing our data for machine learning and start deploying models!","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"598eef","input":"## Titanic Classification Project Part 1","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"6b34e7","input":"#### Discussion Part 2 - Researching Important Concepts in Machine Learning\n\nFirst, go through this [page](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#machine-learning-the-problem-setting).\n\n1. Data Preprocessing: Learn about what [training and testing splits](https://realpython.com/train-test-split-python-data/#supervised-machine-learning-with-train_test_split) are and write about your understanding here about why they are important. \n\n2. Machine Learning model evaluation: Check out what [confusion matrices](https://glassboxmedicine.com/2019/02/17/measuring-performance-the-confusion-matrix/) are and define the following terms in your own words:\n    - Confusion Matrix:\n\n    - True Positives (TP):\n\n    - True Negatives (TN):\n\n    - False Positives (FP):\n\n    - False Negatives (FN):\n\n    - Accuracy (may need to use a different site):\n\n    - Precision:\n\n\n3. Let's say you created a machine learning model that predicts COVID-19 in travelers visiting the USA based on secret government data. This model is created to label the passengers as positive (1) if the model predicts them as positive for COVID-19, and negative (0) if the model classifies them as negative. Describe the implications of the following confusion matrix scenarios for each passenger (the first one is done as an example):\n    - True Positive (TP): The model determines the passenger to be positive for COVID-19, and the passenger actually is positive for COVID-19. The passenger will need to obtain a negative COVID test to enter the United States.\n    - True Negative (TN): \n    - False Positive (FP): \n    - False Negative (FN): \n\n4. Which scenario in part 3 seems the worst possible scenario to you and why? Which categories (TP, TN, FP, FN) should we try to decrease, and which categories should we try to increase by improving our model?\n<br>\n\n\n5. Let's say you get hired by your local mall to to make a machine learning model that detects credit card fraud. What type of data would you need to create a supervised learning model to predict credit card fraud, and would the model be a classification model or a regression model?\n","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"6dcc24","input":"### Part 4: Discuss Machine Learning: Intro to Supervised Machine Learning","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"714f62","input":"## Problem 5: Evaluation\nFor validation in a classification problem, we like to use a few different metrics, some of which we have already talked about! Some popular metrics are as follows:\n- Confusion Matrix **\n- Confusion Matrix Metrics (Accuracy, Precision, Recall, F1-Score) **\n- Classification Report **\n- ROC Curve\n- [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html) scores to be wary of overfitting and let each data point serve as a training and testing point. \n\nOur job here will be to build confusion [matrices](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) with `seaborn` [heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for each of our models, as well as a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) since these metrics will do a good enough job explaining our models' prediction ability. \n\n##### Model 1 - K Nearest Neighbors","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"76f497","input":"### Problem 1: Load and understand the key aspects of the data\n\nYour first task is to conduct some preprocessing steps and provide comments in your code to describe the following:\n1. Load the `titanic_data.csv` file into a pandas data frame.\n2. Find out how many passengers are in the file. Th\n3. List all of the \"features\" in the file.\n4. Use pandas' .describe() function to learn about the dataset.\n5. Use pandas' .head() function to check out the first few rows.\n6. Use pandas' .tail() function to check out the last few rows.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"7b1a07","input":"Step 6: Write hypothesis here using markdown: not enough information yet to make a good conclusion","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"80d5ac","input":"#### Notes from this section:\nwrite your notes here using markdown (double click on this slide)\nhello the mean is lower than 0.5 :(","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"927bed","input":"#### Model 1 - K Nearest Neighbors Model\nCreate a K Nearest Neighbors model object for each value of k between 1 and 30 and find out what the best value of k is. ","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"969c26","input":"#### Extra Part\n- Now, create a final dataframe to train on based off of the features we think are useful for machine learning. ","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"9a186d","input":"#### Imports (libraries we will learn and use!)","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"a7b87d","input":"Step 4: Write a hypothesis here using markdown. You can make an educated guess toward how age and survival are related. slightly depending on the range for example, people whos ages are from maybe 5-12 would be considered children but since they can move themselves, they would have the highest probability of survival.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"aa82ea","input":"### Introduction to the Project - Videos and Background\n\nThe RMS Titanic sank in the early morning hours of 15 April 1912 in the North Atlantic Ocean, four days into her maiden voyage from Southampton to New York City. The largest ocean liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg at around 23:40 (ship's time) on Sunday, 14 April 1912. Her sinking two hours and forty minutes later at 02:20 (ship's time; 05:18 GMT) on Monday, 15 April, resulted in the deaths of more than 1,500 people, making it one of the deadliest peacetime maritime disasters in history. [(wikipedia)](https://en.wikipedia.org/wiki/Sinking_of_the_Titanic)\n\nSetting the stage with some videos: https://www.youtube.com/watch?v=3lyiZMeTKIo and\nhttps://www.youtube.com/watch?v=ItjXTieWKyI \n\nThe files uploaded in this folder contain data for real Titanic passengers. Each row represents one person. The columns (features) describe different attributes about the person including whether they survived (`0=No`), their age, their passenger-class (`1=1st Class, Upper`), gender, and the fare they paid (Â£s*).\n\n**Overall Goal of this Project (what we are working toward)**: perform machine learning on the titanic dataset to predict which passengers will survive based on the information we have on each passenger.  \n\n\nTechnical Goals of part 1 of the project:\n* Understand all of the imports, and start checking out new libraries [sklearn](https://scikit-learn.org/stable/), [matplotlib](https://matplotlib.org/), and [seaborn](https://seaborn.pydata.org/).\n* Learn how to get to know and describe a dataset using [pandas](https://pandas.pydata.org/).\n* Become a pandas expert in data exploration.\n* Learn basic visualization in statistics using [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/).\n* Learn an important component of data preprocessing in supervised machine learning: [dummy variables](https://stattrek.com/multiple-regression/dummy-variables.aspx)\n* Learn how to write in markdown, which will help you learn Latex and write awesome github readmes in the future: [markdown] (https://www.markdownguide.org/basic-syntax/)","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"b2e51e","input":"Now that we are pandas experts, we will take a step away from coding to discuss what Machine Learning is. The machine learning library we are going to be using is called [scikit-learn](https://scikit-learn.org/stable/). Our goal  for this week is to start learning about machine learning! Here are some questions we will research and address in class:\n \n1. What are the three types of machine learning and what are the differences between them?\n\nSupervised Learning: Labeled data and we have expected results to work with\n\nUnsupervised Learning: Data is not labeled and have no expected results\n\nDeep Learning: You use neural networks and it is more complex problem setting (pattern recognition, reinforcement learning)\n\n2. What are some uses of machine learning? What type of machine learning seems the most interesting to you?\n\nAd recommendations, face detection through cameras. Deep learning seems most interesting since my mind also seems to like patterns such as tiles on the floor.\n\n3. We will be using supervised machine learning for this project. Why do you think this dataset is a good candidate for supervised machine learning?\n\nAll the data is labeled(age, sex, Pclass, fare...) = we have the expected results(survived column)\n\n4. Check out this [example](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#machine-learning-the-problem-setting) and machine learning guide on scikit-learn. This page introduces some awesome vocabulary and talks about why we use machine learning and different types of learning problems. Which type of supervised learning problem does our project fall into?\n\nclassification since we are classifying survived or not survived.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"ba08a8","input":"#### Splitting our data into training and testing\nSplit your data into training and testing pieces for training/fitting the model on the training data and later evaluating the model on the testing data.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"bb7f73","input":"##### Which model is best?\nAfter performing model evaluation on the three models, which model does the best job of predicting survivability on the titanic in your opinion? Write your answer in markdown here:\n<br>\n\n\n\n","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"c1c656","input":"## Problem 6: Bonus Model!","pos":60,"type":"cell"}
{"cell_type":"markdown","id":"d94ef7","input":"#### Notes from this section: \nwrite your notes here using markdown.","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"e58e11","input":"### Problem 3: preprocess the data\nThe `Sex` column is categorical, meaning its data are separable into groups, but not numerical. To be able to work with this data, we need numbers, so you task is to transform the `Sex` column into numerical data with pandas' `get_dummies` feature and remove the original categorical `Sex` column. We want this column to be only zeros and ones so we can perform calculations on it for Machine Learning. In this case 0 could represent female and 1 could represent male, or vice versa!\n\nHint: I used pandas' `.replace()` function for this problem.\n","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"e75497","input":"##### Plotting\nNow that you have fit the model, we will plot the 10th decision tree in tree_model's random forest, and the code is given to you below. Note: this code is adapted from [this article](https://mljar.com/blog/visualize-tree-from-random-forest/).","pos":63,"type":"cell"}
{"cell_type":"markdown","id":"e89b4e","input":"#### Model 4 Decision Trees and Random Forests\nNow we will look into a different way of thinking for classification problems: decision trees and random forests, which are incredible ways to understand a dataset. These videos get a little math-y but I think they do a good job explaining decision trees and random forest and why they are great ways to break down a classification problem.\n- Homework: Check out this video for [Decision Trees](https://www.youtube.com/watch?v=ZVR2Way4nwQ)\n- Homework: Check out this video for [Random Forests](https://www.youtube.com/watch?v=v6VJ2RO66Ag)\n- Homework: Read up on the SK Learn Documentation for [Random Forest Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n- Homework: Fit a Random Forest Classifier on the data and name your classifier `tree_model`. Set parameters for the model to have have 100 estimators, and a max depth of 4.\n- Homework: After fitting the model, use the code block below to plot your n-th tree.\n","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"f3d785","input":"##### Model 2 - Support Vector Machines","pos":51,"type":"cell"}
{"id":0,"time":1644797189621,"type":"user"}
{"last_load":1644797190608,"type":"file"}